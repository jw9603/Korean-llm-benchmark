# LLM Evaluation - Korean Benchmarks

í•œêµ­ì–´ LLM ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ë„êµ¬. vLLM ë˜ëŠ” OpenAI-compatible APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.

## Updates

- **2026-01-20**: ì½”ë“œ ë¦¬íŒ©í† ë§ - ê³µí†µ ëª¨ë“ˆ(`core.py`) ë¶„ë¦¬, ì¤‘ë³µ ì½”ë“œ ì œê±°
- **2026-01-16**: CoT ì •ë‹µ ì¶”ì¶œ ê°œì„  - ë‹¤ì–‘í•œ ë‹µë³€ í˜•ì‹ ì§€ì› (Final Answer, ì •ë‹µ ë“±)
- **2026-01-15**: kanana-2-30b-a3b-thinking-2601 ëª¨ë¸ í‰ê°€ ì™„ë£Œ
- **2026-01-14**: MMLU-Pro ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ (ì˜ì–´, 0-shot/5-shot CoT)
- **2026-01-13**: openai/gpt-oss-120b ëª¨ë¸ í‰ê°€ ì™„ë£Œ
- **2026-01-09**: [VAETKI-20B-A2B](https://huggingface.co/nc-ai-consortium/VAETKI-20B-A2B) ëª¨ë¸ í‰ê°€ ì™„ë£Œ, LotteGPT í‰ê°€ ì§„í–‰ì¤‘(API ì—”ë“œ)
- **2026-01-08**: KorMedMCQA ë°ì´í„°ì…‹ ì¶”ê°€ (í•œêµ­ ì˜ë£Œ ë©´í—ˆì‹œí—˜, 4ê°œ ì„œë¸Œì…‹)
- **2026-01-08**: `datasets` â†’ `dataset_configs` í´ë”ëª… ë³€ê²½ (HuggingFace datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ í•´ê²°)
- **2026-01-08**: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜ í†µì¼ (`llm_evaluation/` ì•ˆì—ì„œ ì‹¤í–‰)
- **2026-01-08**: VAETKI MoE ëª¨ë¸ ì§€ì›ì„ ìœ„í•œ ë³„ë„ í™˜ê²½ ì„¤ì • ì¶”ê°€ (`requirements-vaetki.txt`)
- **2026-01-07**: KorMedMCQA ë°ì´í„°ì…‹ ì¶”ê°€ (í•œêµ­ ì˜ë£Œ ë©´í—ˆì‹œí—˜, 4ê°œ ì„œë¸Œì…‹)
- **2026-01-07**: OpenAI Chat API ë°±ì—”ë“œ ì¶”ê°€ (GPT-4o ë“± ì§€ì›)
- **2026-01-06**: ë¦¬ë”ë³´ë“œ ìë™ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€

<details>
<summary>click for more news</summary>

- **2026-01-05**: Ko-MuSR ë°ì´í„°ì…‹ ì¶”ê°€ (ë‹¤ë‹¨ê³„ ì¶”ë¡ )
- **2026-01-04**: HRM8K ìˆ˜í•™ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€
- **2026-01-03**: ì´ˆê¸° ë²„ì „ ë¦´ë¦¬ì¦ˆ

</details>

## ì§€ì› ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹ | ì„¤ëª… | ìœ í˜• | ì„ íƒì§€ | ë©”íŠ¸ë¦­ |
|----------|------|------|--------|--------|
| **kmmlu** | í•œêµ­ì–´ MMLU | MCQA (4ì§€ì„ ë‹¤) | A,B,C,D | Accuracy |
| **kmmlu_pro** | í•œêµ­ì–´ MMLU Pro | MCQA (5ì§€ì„ ë‹¤) | 1,2,3,4,5 | Accuracy |
| **mmlu_pro(zero)** | MMLU-Pro (ì˜ì–´, 0-shot) | Generation (CoT) | A~J | Exact Match |
| **mmlu_pro(five)** | MMLU-Pro (ì˜ì–´, 5-shot) | Generation (CoT) | A~J | Exact Match |
| **csatqa** | í•œêµ­ ìˆ˜ëŠ¥ êµ­ì–´ | MCQA (5ì§€ì„ ë‹¤) | 1,2,3,4,5 | Accuracy |
| **haerae** | í•œêµ­ ë¬¸í™”/ì‚¬íšŒ | MCQA (5ì§€ì„ ë‹¤) | A,B,C,D,E | Accuracy |
| **hrm8k** | í•œêµ­ì–´ ìˆ˜í•™ ë¬¸ì œ | Generation | - | Exact Match |
| **hrm8k_mmmlu** | í•œêµ­ì–´ ìˆ˜í•™ (MMMLU) | Generation | 1,2,3,4 | Exact Match |
| **kobalt** | í•œêµ­ì–´ BALT | MCQA (10ì§€ì„ ë‹¤) | A~J | Accuracy |
| **click** | CLIcK ë²¤ì¹˜ë§ˆí¬ | MCQA | A,B,C,D,E | Accuracy |
| **ko_musr_mm** | í•œêµ­ì–´ MuSR (Murder Mystery) | Generation | 1,2,3 | Accuracy |
| **ko_musr_op** | í•œêµ­ì–´ MuSR (Object Placement) | Generation | 1,2,3 | Accuracy |
| **ko_musr_ta** | í•œêµ­ì–´ MuSR (Team Allocation) | Generation | 1,2,3 | Accuracy |
| **kormedmcqa** | í•œêµ­ ì˜ë£Œ ë©´í—ˆì‹œí—˜ | MCQA (5ì§€ì„ ë‹¤) | A,B,C,D,E | Accuracy |

### ë°ì´í„°ì…‹ ë¼ì´ì„¼ìŠ¤

| ë°ì´í„°ì…‹ | ë¼ì´ì„¼ìŠ¤ | ìƒì—…ì  ì‚¬ìš© |
|----------|----------|-------------|
| [kmmlu](https://huggingface.co/datasets/HAERAE-HUB/KMMLU) | CC-BY-SA-4.0 | âœ… ê°€ëŠ¥ |
| [kmmlu_pro](https://huggingface.co/datasets/LGAI-EXAONE/KMMLU-Pro) | CC-BY-NC-ND-4.0 | âŒ ë¶ˆê°€ |
| [csatqa](https://huggingface.co/datasets/HAERAE-HUB/csatqa) | ì—°êµ¬ ëª©ì ë§Œ í—ˆìš© | âŒ ë¶ˆê°€ |
| [haerae](https://huggingface.co/datasets/HAERAE-HUB/HAE_RAE_BENCH_1.0) | CC-BY-NC-ND | âŒ ë¶ˆê°€ |
| [hrm8k, hrm8k_mmmlu](https://huggingface.co/datasets/HAERAE-HUB/HRM8K) | MIT | âœ… ê°€ëŠ¥ |
| [kobalt](https://huggingface.co/datasets/snunlp/KoBALT-700) | CC-BY-NC-4.0 | âŒ ë¶ˆê°€ |
| [click](https://huggingface.co/datasets/EunsuKim/CLIcK) | ëª…ì‹œ ì•ˆ ë¨ | âš ï¸ í™•ì¸ í•„ìš” |
| [ko_musr_*](https://huggingface.co/datasets/thunder-research-group/SNU_Ko-MuSR) | MIT | âœ… ê°€ëŠ¥ |
| [kormedmcqa](https://huggingface.co/datasets/sean0042/KorMedMCQA) | CC-BY-NC-2.0 | âŒ ë¶ˆê°€ |
| [mmlu_pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro) | MIT | âœ… ê°€ëŠ¥ |

## ì§€ì› ë°±ì—”ë“œ

| ë°±ì—”ë“œ | ì„¤ëª… | í‰ê°€ ë°©ì‹ |
|--------|------|-----------|
| **vllm** | ë¡œì»¬ GPU (vLLM) | Loglikelihood |
| **openai** | OpenAI Completions API | Loglikelihood |
| **chat** | OpenAI Chat API | Generation |
| **api** | ìë™ ê°ì§€ | ìë™ ì„ íƒ |

## ì„¤ì¹˜

### ê¸°ë³¸ í™˜ê²½ (vLLM 0.13+)

ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ í™˜ê²½ì…ë‹ˆë‹¤.

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
conda create -n llm python=3.11
conda activate llm

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### VAETKI í™˜ê²½ (vLLM 0.11.2)

NC AIì˜ VAETKI MoE ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë³„ë„ í™˜ê²½ì…ë‹ˆë‹¤.

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
conda create -n vaetki python=3.11
conda activate vaetki

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements-vaetki.txt

# VAETKI í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜
git clone --depth 1 --branch dev https://github.com/cyr0930/Megatron-LM-wbl.git
PYTHONPATH="$PWD/Megatron-LM-wbl/vllm_plugin" pip install Megatron-LM-wbl/vllm_plugin
rm -rf Megatron-LM-wbl
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

API ë°±ì—”ë“œ ì‚¬ìš© ì‹œ `.env` íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

```bash
# OpenAI API
OPENAI_API_KEY=sk-xxx

# Friendly AI (K-EXAONE)
FRIENDLI_API_KEY=xxx
```

## ì‚¬ìš©ë²•

> **Note:** `scripts/run_eval.sh` ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë“  í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í†µí•© ìŠ¤í¬ë¦½íŠ¸ (run_eval.sh)

```bash
# ë„ì›€ë§
./llm_evaluation/scripts/run_eval.sh --help

# ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€
./llm_evaluation/scripts/run_eval.sh --model gpt-4o

# ë‹¨ì¼ ë°ì´í„°ì…‹ í‰ê°€
./llm_evaluation/scripts/run_eval.sh --dataset kmmlu --model gpt-4o

# ìƒ˜í”Œ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
./llm_evaluation/scripts/run_eval.sh --dataset hrm8k --model gpt-4o --limit 100
```

### ìŠ¤í¬ë¦½íŠ¸ ì˜µì…˜

| ì˜µì…˜ | ë‹¨ì¶• | ì„¤ëª… |
|------|------|------|
| `--model` | `-m` | ëª¨ë¸ ì´ë¦„ ë˜ëŠ” í”„ë¦¬ì…‹ (í•„ìˆ˜) |
| `--dataset` | `-d` | ë‹¨ì¼ ë°ì´í„°ì…‹ (ì—†ìœ¼ë©´ ì „ì²´ í‰ê°€) |
| `--limit` | `-l` | ìƒ˜í”Œ ìˆ˜ ì œí•œ |
| `--concurrency` | `-c` | ë™ì‹œ ìš”ì²­ ìˆ˜ (API) |
| `--tensor-parallel-size` | | GPU ë³‘ë ¬ ìˆ˜ (vLLM) |
| `--max-model-len` | | ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (vLLM) |
| `--base-url` | | ì»¤ìŠ¤í…€ API URL |
| `--api-key` | | API í‚¤ |
| `--output-dir` | `-o` | ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ |

### ëª¨ë¸ í”„ë¦¬ì…‹

| í”„ë¦¬ì…‹ | ëª¨ë¸ | ë°±ì—”ë“œ |
|--------|------|--------|
| `gpt-5.2` | gpt-5.2-2025-12-11 | OpenAI API |
| `gpt-4o` | gpt-4o | OpenAI API |
| `gpt-4o-mini` | gpt-4o-mini | OpenAI API |
| `k-exaone` | K-EXAONE-236B | Friendly AI |

### ì˜ˆì‹œ

```bash
# API ëª¨ë¸ (í”„ë¦¬ì…‹)
./llm_evaluation/scripts/run_eval.sh --model gpt-4o --dataset kmmlu

# ì»¤ìŠ¤í…€ API ì—”ë“œí¬ì¸íŠ¸
./llm_evaluation/scripts/run_eval.sh \
    --model krevas/gpt-oss-120b \
    --base-url https://api.example.com/v1 \
    --api-key your-key \
    --dataset kobalt

# vLLM ë¡œì»¬ (HuggingFace ëª¨ë¸)
./llm_evaluation/scripts/run_eval.sh \
    --model Qwen/Qwen3-32B \
    --tensor-parallel-size 2

# CLI ì§ì ‘ ì‚¬ìš©
python -m llm_evaluation --backend vllm --model Qwen/Qwen3-32B --datasets kmmlu csatqa
python -m llm_evaluation --backend api --model gpt-4o --datasets kmmlu

# GPT-5/o1/o3 reasoning effort ì˜µì…˜
python -m llm_evaluation --backend api --model gpt-5.2-2025-12-11 --reasoning-effort medium --datasets kmmlu
```


## í‰ê°€ ë°©ì‹

### MCQA (Multiple Choice QA)

**Loglikelihood ë°©ì‹** (vLLM, OpenAI Completions API)
- ê° ì„ íƒì§€ì— ëŒ€í•´ `log P(choice|context)` ê³„ì‚°
- ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì„ íƒì§€ë¥¼ ì •ë‹µìœ¼ë¡œ ì„ íƒ
- lm-evaluation-harness í‘œì¤€ ë°©ì‹
- `acc`: Raw accuracy (log P(choice|context) ê¸°ì¤€)
- `acc_norm`: í† í° ê¸¸ì´ë¡œ ì •ê·œí™”ëœ accuracy
  - ê¸´ ì„ íƒì§€ëŠ” í† í°ì´ ë§ì•„ logprob í•©ì´ ë‚®ì•„ì§€ëŠ” bias ì¡´ì¬
  - `log P / num_tokens`ë¡œ ì •ê·œí™”í•˜ì—¬ ê¸¸ì´ bias ë³´ì •
  - ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ" (3í† í°) vs "ì„œìš¸" (1í† í°) â†’ ì •ê·œí™” ì—†ì´ëŠ” "ì„œìš¸"ì´ ìœ ë¦¬

**Generation ë°©ì‹** (Chat API)
- Chat APIëŠ” prompt logprobsë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ loglikelihood ê³„ì‚° ë¶ˆê°€
- ëŒ€ì‹  ëª¨ë¸ì—ê²Œ ì§ì ‘ ì •ë‹µì„ ìƒì„±í•˜ë„ë¡ ìš”ì²­
- ê°•í™”ëœ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë‹¨ì¼ ë¬¸ì ì‘ë‹µ ìœ ë„:
  ```
  ì§ˆë¬¸: ë‹¤ìŒ ì¤‘ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?
  A. ë¶€ì‚°
  B. ì„œìš¸
  C. ëŒ€ì „
  D. ì¸ì²œ

  ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš” (A, B, C, D ì¤‘ í•˜ë‚˜):

  CRITICAL: You MUST respond with EXACTLY ONE LETTER ONLY (A, B, C, D).
  ABSOLUTELY NO explanations, reasoning, or additional text.
  Just the letter. Period.
  ```
- 9ê°€ì§€ íŒ¨í„´ìœ¼ë¡œ ì‘ë‹µ íŒŒì‹±:
  1. ì²« ì¤„ ë‹¨ì¼ ë¬¸ì (ì˜ˆ: `A`)
  2. `### ANSWER` ì„¹ì…˜
  3. `Answer: A` / `ì •ë‹µ: A` í˜•ì‹
  4. `(A)` ë˜ëŠ” `[A]` í˜•ì‹
  5. `A)` ë˜ëŠ” `A.` í˜•ì‹ (ì¤„ ì‹œì‘)
  6. XML íƒœê·¸ `<answer>A</answer>`
  7. ë§ˆì§€ë§‰ ì¤„ ë‹¨ì¼ ë¬¸ì
  8. ì‘ë‹µ ë‚´ ë‹¨ì¼ ë¬¸ì A/B/C/D
  9. ì •ê·œì‹ fallback
- GPT-4o, GPT-5, K-EXAONE ë“± Chat ì „ìš© ëª¨ë¸ì—ì„œ ì‚¬ìš©

### Generation (ìˆ˜í•™/ì¶”ë¡  ë¬¸ì œ)

ì¼ë¶€ ë°ì´í„°ì…‹ì€ ì„ íƒì§€ê°€ ìˆì§€ë§Œ `generate_until` ë°©ì‹ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤:

| ë°ì´í„°ì…‹ | ì„¤ëª… | ì´ìœ  |
|----------|------|------|
| **hrm8k_mmmlu** | ìˆ˜í•™ 4ì§€ì„ ë‹¤ | `\boxed{N}` í˜•ì‹ìœ¼ë¡œ ìƒì„± |
| **ko_musr_*** | ì¶”ë¡  ë¬¸ì œ | CoT ì¶”ë¡  í›„ "ì •ë‹µ: X" ìƒì„± |
| **mmlu_pro_*** | MMLU-Pro (ì˜ì–´, 0/5-shot) | CoT ì¶”ë¡  í›„ "the answer is (X)" ì¶”ì¶œ |

**ì™œ MCQAê°€ ì•„ë‹Œ Generationì¸ê°€?**
- ë‹¨ìˆœ í™•ë¥  ë¹„êµê°€ ì•„ë‹Œ **ì¶”ë¡  ê³¼ì • ìƒì„±**ì„ ìœ ë„
- í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¨ê³„ë³„ ì¶”ë¡  í›„ ì •ë‹µ ì¶œë ¥ ìš”ì²­
- lm-evaluation-harness ì›ë³¸ ë²¤ì¹˜ë§ˆí¬ í‘œì¤€ ë°©ì‹

**ì •ë‹µ ì¶”ì¶œ ë°©ì‹:**
- ìˆ˜í•™: `\boxed{N}` í˜•ì‹ì—ì„œ ì¶”ì¶œ, ìˆ˜í•™ì  ë™ì¹˜ì„± ë¹„êµ (ì˜ˆ: `0.5` = `1/2`)
- ì¶”ë¡ : "ì •ë‹µ: X" í˜•ì‹ì—ì„œ ì¶”ì¶œ
- MMLU-Pro: `the answer is (X)` íŒ¨í„´ì—ì„œ ë§ˆì§€ë§‰ ë§¤ì¹˜ ì¶”ì¶œ

## CLI ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--backend` | í‰ê°€ ë°±ì—”ë“œ (vllm/api/openai/chat) | vllm |
| `--model` | ëª¨ë¸ ì´ë¦„ | (í•„ìˆ˜) |
| `--datasets` | í‰ê°€í•  ë°ì´í„°ì…‹ | ì „ì²´ |
| `--output-dir` | ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ | ./llm_evaluation/results |
| `--tensor-parallel-size` | GPU ë¶„ì‚° ìˆ˜ (vLLM) | 1 |
| `--gpu-memory-utilization` | GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (vLLM) | 0.9 |
| `--max-model-len` | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (vLLM) | auto |
| `--base-url` | API ë² ì´ìŠ¤ URL | - |
| `--api-key` | API í‚¤ | OPENAI_API_KEY |
| `--reasoning-effort` | GPT-5/o1/o3 reasoning ìˆ˜ì¤€ (low/medium/high) | None |

## ê²°ê³¼ ì¶œë ¥

```
llm_evaluation/results/
â”œâ”€â”€ {model_id}_results.json           # ìš”ì•½ ê²°ê³¼
â”œâ”€â”€ {model_id}_{dataset}_details.json # ìƒì„¸ ê²°ê³¼
â””â”€â”€ leaderboard.md                    # ë¦¬ë”ë³´ë“œ

# model_id ì˜ˆì‹œ:
# - vLLM: "Qwen3-32B"
# - Chat API: "gpt-4o"
# - reasoning_effort ì‚¬ìš© ì‹œ: "gpt-5.2-2025-12-11(medium)"
```

## ê°œë°œ ìƒíƒœ

| ê¸°ëŠ¥ | ìƒíƒœ |
|------|------|
| vLLM ë°±ì—”ë“œ | âœ… ì™„ë£Œ |
| OpenAI Completions API | âœ… ì™„ë£Œ |
| OpenAI Chat API | âœ… ì™„ë£Œ |
| K-EXAONE ì§€ì› | â¸ï¸ ì ì • ì¤‘ë‹¨ |
| Streamlit ëŒ€ì‹œë³´ë“œ | ğŸš§ ë¯¸ì™„ì„± |

### ì°¸ê³ : skt/A.X-3.1 í…ŒìŠ¤íŠ¸ í™˜ê²½

`skt/A.X-3.1` ëª¨ë¸ì€ H100 80GB 1ì¥ì— ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì•„ `max_model_len=8192`ë¡œ ì œí•œí•˜ì—¬ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.

```bash
./llm_evaluation/scripts/run_eval.sh skt/A.X-3.1 1 8192
```

### ì°¸ê³ : K-EXAONE (Friendly AI)

K-EXAONE ëª¨ë¸ì€ Friendly AI serverless APIë¥¼ í†µí•´ ì§€ì›ë˜ë‚˜, ë„ˆë¬´ ëŠë ¤ì„œ.. í˜„ì¬ **ì ì • ì¤‘ë‹¨** ìƒíƒœì…ë‹ˆë‹¤.

**ë¬¸ì œì :**
- ì‘ë‹µ ì†ë„ê°€ ë§¤ìš° ëŠë¦¼ (~17ì´ˆ/ìš”ì²­, GPT-4o ëŒ€ë¹„ ì•½ 35ë°°)
- Rate limitingìœ¼ë¡œ ëŒ€ê·œëª¨ í‰ê°€ ë¶ˆê°€ (429 ì—ëŸ¬ ë¹ˆë°œ)
- Serverless íŠ¹ì„±ìƒ cold start ì§€ì—° ë°œìƒ

## TODO

- [ ] [Ko-IFEval](https://huggingface.co/datasets/allganize/IFEval-Ko) ë°ì´í„°ì…‹ ì¶”ê°€í•˜ê¸°
- [ ] [KBL](https://huggingface.co/datasets/lbox/kbl) ë°ì´í„°ì…‹ ì¶”ê°€í•˜ê¸°
- [x] [KorMedMCQA](https://huggingface.co/datasets/sean0042/KorMedMCQA) ë°ì´í„°ì…‹ ì¶”ê°€í•˜ê¸°
- [ ] Multi-GPU í™˜ê²½ì—ì„œ ë” í° ëª¨ë¸ ê²°ê³¼ ì˜¬ë¦¬ê¸°
- [ ] ë‹¤ì–‘í•œ API ëª¨ë¸ í…ŒìŠ¤íŠ¸
- [ ] Streamlit ëŒ€ì‹œë³´ë“œ êµ¬í˜„

## References

- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - EleutherAI LLM í‰ê°€ í”„ë ˆì„ì›Œí¬
- [evaluate-llm-on-korean-dataset](https://github.com/daekeun-ml/evaluate-llm-on-korean-dataset) - í•œêµ­ì–´ ë°ì´í„°ì…‹ LLM í‰ê°€

<details>
<summary>ë²¤ì¹˜ë§ˆí¬</summary>

```bibtex
@misc{son2024kmmlumeasuringmassivemultitask,
      title={KMMLU: Measuring Massive Multitask Language Understanding in Korean},
      author={Guijin Son and Hanwool Lee and Sungdong Kim and Seungone Kim and Niklas Muennighoff and Taekyoon Choi and Cheonbok Park and Kang Min Yoo and Stella Biderman},
      year={2024},
      eprint={2402.11548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11548},
}

@misc{son2024haeraebenchevaluationkorean,
      title={HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models},
      author={Guijin Son and Hanwool Lee and Suwan Kim and Huiseo Kim and Jaecheol Lee and Je Won Yeom and Jihyu Jung and Jung Woo Kim and Songseong Kim},
      year={2024},
      eprint={2309.02706},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.02706},
}

@misc{kim2024click,
      title={CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean},
      author={Eunsu Kim and Juyoung Suk and Philhoon Oh and Haneul Yoo and James Thorne and Alice Oh},
      year={2024},
      eprint={2403.06412},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{ko2025hrm8k,
      title={HRM8K: A Bilingual Math Reasoning Benchmark for Korean and English},
      author={Hyunwoo Ko and Guijin Son and Dasol Choi},
      year={2025},
      eprint={2501.02448},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.02448},
}

@misc{lee2025kobalt,
      title={KoBALT: A Benchmark for Evaluating Korean Linguistic Phenomena in Large Language Models},
      author={Dohyun Lee and Seunghyun Hwang and Seungtaek Choi and Hwisang Jeon and Sohyun Park and Sungjoon Park and Yungi Kim},
      year={2025},
      eprint={2505.16125},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.16125},
}

@misc{park2025komusrmultistepsoftreasoning,
      title={Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean},
      author={Chanwoo Park and Suyoung Park and JiA Kang and Jongyeon Park and Sangho Kim and Hyunji M. Park and Sumin Bae and Mingyu Kang and Jaejin Lee},
      year={2025},
      eprint={2510.24150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.24150},
}

@misc{hong2025kmmlureduxkmmluproprofessionalkorean,
      title={From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation},
      author={Seokhee Hong and Sunkyoung Kim and Guijin Son and Soyeon Kim and Yeonjung Hong and Jinsik Lee},
      year={2025},
      eprint={2507.08924},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.08924},
}

@misc{kweon2024kormedmcqamultichoicequestionanswering,
      title={KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations},
      author={Sunjun Kweon and Byungjin Choi and Gyouk Chu and Junyeong Song and Daeun Hyeon and Sujin Gan and Jueon Kim and Minkyu Kim and Rae Woong Park and Edward Choi},
      year={2024},
      eprint={2403.01469},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.01469},
}

@misc{wang2024mmluprorobustchallengingmultitask,
      title={MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
      author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
      year={2024},
      eprint={2406.01574},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01574},
}

% csatqa: https://huggingface.co/datasets/HAERAE-HUB/csatqa (ë…¼ë¬¸ ì—†ìŒ)
```

</details>
